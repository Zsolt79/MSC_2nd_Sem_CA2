{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da4b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField, FloatType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0f742a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:34:53,859 WARN util.Utils: Your hostname, muhammad-Vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2023-05-21 17:34:53,864 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.3-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hduser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hduser/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-604c68fa-7df8-4a34-b4e9-009abd161666;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;2.4.2 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.12.5 in central\n",
      ":: resolution report :: resolve 930ms :: artifacts dl 30ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#mongo-java-driver;3.12.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;2.4.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-604c68fa-7df8-4a34-b4e9-009abd161666\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/41ms)\n",
      "2023-05-21 17:34:57,577 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "#spark = SparkSession.builder.appName(\"TextProcessing\").getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TextProcessing\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:2.4.2\") \\\n",
    "    .getOrCreate()\n",
    "# A JSON dataset is pointed to by path.\n",
    "# Move the 'people.json' file to HDFS path '/user1'\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "path = \"/user1/Trump_Tweets.json\"\n",
    "twitterDF = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2fb46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Client: string (nullable = true)\n",
      " |-- Client Simplified: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813a431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+-----------+--------------------+\n",
      "|             Client|  Client Simplified|      Date|       Time|               Tweet|\n",
      "+-------------------+-------------------+----------+-----------+--------------------+\n",
      "| Twitter for iPhone| Twitter for iPhone|01/27/2017| 6:46:22 PM| I promise that o...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/27/2017| 5:00:47 PM| Congratulations ...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/27/2017| 3:20:15 PM| Statement on Int...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/27/2017|11:30:29 AM| .@VP Mike Pence ...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/27/2017|11:27:02 AM| The #MarchForLif...|\n",
      "|Twitter for Android|Twitter for Android|01/27/2017| 8:19:10 AM| Mexico has taken...|\n",
      "|Twitter for Android|Twitter for Android|01/27/2017| 8:12:52 AM| Look forward to ...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/26/2017| 6:53:37 PM| Miami-Dade Mayor...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/26/2017| 6:45:28 PM| Will be intervie...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/26/2017| 2:21:17 PM| Spoke at the Con...|\n",
      "|Twitter for Android|Twitter for Android|01/26/2017| 8:55:03 AM| of jobs and comp...|\n",
      "|Twitter for Android|Twitter for Android|01/26/2017| 8:51:46 AM| The U.S. has a 6...|\n",
      "|Twitter for Android|Twitter for Android|01/26/2017| 6:04:24 AM| Ungrateful TRAIT...|\n",
      "|Twitter for Android|Twitter for Android|01/25/2017| 9:48:25 PM| Interview with D...|\n",
      "|Twitter for Android|Twitter for Android|01/25/2017| 9:45:32 PM| \"@romoabcnews: ....|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/25/2017| 9:14:56 PM| As your Presiden...|\n",
      "| Twitter Web Client| Twitter Web Client|01/25/2017| 7:03:33 PM| Beginning today,...|\n",
      "| Twitter for iPhone| Twitter for iPhone|01/25/2017| 5:05:59 PM| I will be interv...|\n",
      "|Twitter for Android|Twitter for Android|01/25/2017| 7:17:01 AM| I will be making...|\n",
      "|Twitter for Android|Twitter for Android|01/25/2017| 7:13:46 AM| even, those regi...|\n",
      "+-------------------+-------------------+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04555781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "selected_columns = [\"Date\", \"Time\", \"Tweet\"]\n",
    "twitterDF = twitterDF.select(*selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3eec211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+\n",
      "|      Date|       Time|               Tweet|\n",
      "+----------+-----------+--------------------+\n",
      "|01/27/2017| 6:46:22 PM| I promise that o...|\n",
      "|01/27/2017| 5:00:47 PM| Congratulations ...|\n",
      "|01/27/2017| 3:20:15 PM| Statement on Int...|\n",
      "|01/27/2017|11:30:29 AM| .@VP Mike Pence ...|\n",
      "|01/27/2017|11:27:02 AM| The #MarchForLif...|\n",
      "|01/27/2017| 8:19:10 AM| Mexico has taken...|\n",
      "|01/27/2017| 8:12:52 AM| Look forward to ...|\n",
      "|01/26/2017| 6:53:37 PM| Miami-Dade Mayor...|\n",
      "|01/26/2017| 6:45:28 PM| Will be intervie...|\n",
      "|01/26/2017| 2:21:17 PM| Spoke at the Con...|\n",
      "|01/26/2017| 8:55:03 AM| of jobs and comp...|\n",
      "|01/26/2017| 8:51:46 AM| The U.S. has a 6...|\n",
      "|01/26/2017| 6:04:24 AM| Ungrateful TRAIT...|\n",
      "|01/25/2017| 9:48:25 PM| Interview with D...|\n",
      "|01/25/2017| 9:45:32 PM| \"@romoabcnews: ....|\n",
      "|01/25/2017| 9:14:56 PM| As your Presiden...|\n",
      "|01/25/2017| 7:03:33 PM| Beginning today,...|\n",
      "|01/25/2017| 5:05:59 PM| I will be interv...|\n",
      "|01/25/2017| 7:17:01 AM| I will be making...|\n",
      "|01/25/2017| 7:13:46 AM| even, those regi...|\n",
      "+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb0ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text processing\n",
    "def process_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs using regular expressions\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove mentions using regular expressions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    \n",
    "    # Remove hashtags using regular expressions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Register the text processing function as a UDF\n",
    "process_text_udf = udf(process_text, StringType())\n",
    "\n",
    "# Apply the text processing UDF to the \"Tweet\" column\n",
    "df = twitterDF.withColumn(\"clean_tweet\", process_text_udf(twitterDF[\"Tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db684f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current column names\n",
    "current_columns = df.columns\n",
    "\n",
    "# Create a new DataFrame with lowercase column names\n",
    "df = df.toDF(*[col.lower() for col in current_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26743569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+--------------------+\n",
      "|      date|       time|               tweet|         clean_tweet|\n",
      "+----------+-----------+--------------------+--------------------+\n",
      "|01/27/2017| 6:46:22 PM| I promise that o...|i promise that ou...|\n",
      "|01/27/2017| 5:00:47 PM| Congratulations ...|congratulations s...|\n",
      "|01/27/2017| 3:20:15 PM| Statement on Int...|statement on inte...|\n",
      "|01/27/2017|11:30:29 AM| .@VP Mike Pence ...|mike pence will b...|\n",
      "|01/27/2017|11:27:02 AM| The #MarchForLif...|the is so importa...|\n",
      "|01/27/2017| 8:19:10 AM| Mexico has taken...|mexico has taken ...|\n",
      "|01/27/2017| 8:12:52 AM| Look forward to ...|look forward to s...|\n",
      "|01/26/2017| 6:53:37 PM| Miami-Dade Mayor...|miamidade mayor d...|\n",
      "|01/26/2017| 6:45:28 PM| Will be intervie...|will be interview...|\n",
      "|01/26/2017| 2:21:17 PM| Spoke at the Con...|spoke at the cong...|\n",
      "|01/26/2017| 8:55:03 AM| of jobs and comp...|of jobs and compa...|\n",
      "|01/26/2017| 8:51:46 AM| The U.S. has a 6...|the us has a 60 b...|\n",
      "|01/26/2017| 6:04:24 AM| Ungrateful TRAIT...|ungrateful traito...|\n",
      "|01/25/2017| 9:48:25 PM| Interview with D...|interview with da...|\n",
      "|01/25/2017| 9:45:32 PM| \"@romoabcnews: ....|first interview s...|\n",
      "|01/25/2017| 9:14:56 PM| As your Presiden...|as your president...|\n",
      "|01/25/2017| 7:03:33 PM| Beginning today,...|beginning today t...|\n",
      "|01/25/2017| 5:05:59 PM| I will be interv...|i will be intervi...|\n",
      "|01/25/2017| 7:17:01 AM| I will be making...|i will be making ...|\n",
      "|01/25/2017| 7:13:46 AM| even, those regi...|even those regist...|\n",
      "+----------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "582393e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Connect to the MongoDB server\n",
    "client = MongoClient('mongodb://localhost:27017')  # Replace 'localhost' with your MongoDB server address\n",
    "\n",
    "# Access the database and collection\n",
    "db = client['sentiment']  # Replace 'sentiment' with your database name\n",
    "collection = db['sentiment_collection']  # Replace 'sentiment_collection' with your collection name\n",
    "\n",
    "# Convert DataFrame to RDD and collect all rows\n",
    "rows = df.rdd.collect() #!!!!here df has to be new_df if everything goes back!!!!\n",
    "\n",
    "# Insert the dataset into the collection\n",
    "for row in rows:\n",
    "    collection.insert_one(row.asDict())\n",
    "\n",
    "# Close the MongoDB connection\n",
    "#client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20079ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+---------------+---------------+\n",
      "|      date|       time|         clean_tweet|sentiment_score|sentiment_label|\n",
      "+----------+-----------+--------------------+---------------+---------------+\n",
      "|01/27/2017| 6:46:22 PM|i promise that ou...|         0.3182|       Positive|\n",
      "|01/27/2017| 5:00:47 PM|congratulations s...|         0.5994|       Positive|\n",
      "|01/27/2017| 3:20:15 PM|statement on inte...|            0.0|        Neutral|\n",
      "|01/27/2017|11:30:29 AM|mike pence will b...|         0.4019|       Positive|\n",
      "|01/27/2017|11:27:02 AM|the is so importa...|         0.6207|       Positive|\n",
      "|01/27/2017| 8:19:10 AM|mexico has taken ...|         0.0552|       Positive|\n",
      "|01/27/2017| 8:12:52 AM|look forward to s...|        -0.1779|       Negative|\n",
      "|01/26/2017| 6:53:37 PM|miamidade mayor d...|         0.5106|       Positive|\n",
      "|01/26/2017| 6:45:28 PM|will be interview...|         0.4939|       Positive|\n",
      "|01/26/2017| 2:21:17 PM|spoke at the cong...|         0.2023|       Positive|\n",
      "|01/26/2017| 8:55:03 AM|of jobs and compa...|        -0.5994|       Negative|\n",
      "|01/26/2017| 8:51:46 AM|the us has a 60 b...|        -0.4019|       Negative|\n",
      "|01/26/2017| 6:04:24 AM|ungrateful traito...|        -0.9062|       Negative|\n",
      "|01/25/2017| 9:48:25 PM|interview with da...|         0.4939|       Positive|\n",
      "|01/25/2017| 9:45:32 PM|first interview s...|            0.0|        Neutral|\n",
      "|01/25/2017| 9:14:56 PM|as your president...|         0.1027|       Positive|\n",
      "|01/25/2017| 7:03:33 PM|beginning today t...|         0.4215|       Positive|\n",
      "|01/25/2017| 5:05:59 PM|i will be intervi...|            0.0|        Neutral|\n",
      "|01/25/2017| 7:17:01 AM|i will be making ...|         0.5574|       Positive|\n",
      "|01/25/2017| 7:13:46 AM|even those regist...|        -0.4588|       Negative|\n",
      "+----------+-----------+--------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Retrieve data from MongoDB collection, excluding the \"_id\" field\n",
    "data = list(collection.find({}, {\"_id\": 0}))\n",
    "\n",
    "# Create a DataFrame from the retrieved data\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Apply sentiment analysis using NLTK\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)['compound']\n",
    "    \n",
    "    if sentiment_score > 0:\n",
    "        sentiment_label = \"Positive\"\n",
    "    elif sentiment_score < 0:\n",
    "        sentiment_label = \"Negative\"\n",
    "    else:\n",
    "        sentiment_label = \"Neutral\"\n",
    "    \n",
    "    return sentiment_score, sentiment_label\n",
    "\n",
    "# Define the schema for the sentiment analysis result\n",
    "sentiment_schema = StructType([\n",
    "    StructField(\"sentiment_score\", FloatType(), nullable=False),\n",
    "    StructField(\"sentiment_label\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "sentiment_udf = udf(analyze_sentiment, sentiment_schema)\n",
    "\n",
    "# Apply sentiment analysis and create new columns for sentiment score and label\n",
    "df = df.withColumn(\"sentiment\", sentiment_udf(df[\"clean_tweet\"]))\n",
    "df = df.withColumn(\"sentiment_score\", df[\"sentiment\"][\"sentiment_score\"])\n",
    "df = df.withColumn(\"sentiment_label\", df[\"sentiment\"][\"sentiment_label\"])\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = [\"date\", \"time\", \"clean_tweet\", \"sentiment_score\", \"sentiment_label\"]\n",
    "df = df.select(*selected_columns)\n",
    "\n",
    "# Show the DataFrame with sentiment analysis results\n",
    "df.show()\n",
    "\n",
    "# Close the MongoDB connection\n",
    "#client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "062ceaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|sentiment_label|tweet_count|\n",
      "+---------------+-----------+\n",
      "|       Positive|     125132|\n",
      "|        Neutral|      36785|\n",
      "|       Negative|      50778|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by sentiments and count the number of tweets in each sentiment category\n",
    "sentiment_counts = df.groupBy(\"sentiment_label\").count().withColumnRenamed(\"count\", \"tweet_count\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "sentiment_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2403f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+---------------+---------------+----+-----+----+---+----+\n",
      "|      date|               time|         clean_tweet|sentiment_score|sentiment_label|year|month|week|day|hour|\n",
      "+----------+-------------------+--------------------+---------------+---------------+----+-----+----+---+----+\n",
      "|2017-01-27|1970-01-01 18:46:22|i promise that ou...|         0.3182|       Positive|2017|    1|   4| 27|  18|\n",
      "|2017-01-27|1970-01-01 17:00:47|congratulations s...|         0.5994|       Positive|2017|    1|   4| 27|  17|\n",
      "|2017-01-27|1970-01-01 15:20:15|statement on inte...|            0.0|        Neutral|2017|    1|   4| 27|  15|\n",
      "|2017-01-27|1970-01-01 11:30:29|mike pence will b...|         0.4019|       Positive|2017|    1|   4| 27|  11|\n",
      "|2017-01-27|1970-01-01 11:27:02|the is so importa...|         0.6207|       Positive|2017|    1|   4| 27|  11|\n",
      "|2017-01-27|1970-01-01 08:19:10|mexico has taken ...|         0.0552|       Positive|2017|    1|   4| 27|   8|\n",
      "|2017-01-27|1970-01-01 08:12:52|look forward to s...|        -0.1779|       Negative|2017|    1|   4| 27|   8|\n",
      "|2017-01-26|1970-01-01 18:53:37|miamidade mayor d...|         0.5106|       Positive|2017|    1|   4| 26|  18|\n",
      "|2017-01-26|1970-01-01 18:45:28|will be interview...|         0.4939|       Positive|2017|    1|   4| 26|  18|\n",
      "|2017-01-26|1970-01-01 14:21:17|spoke at the cong...|         0.2023|       Positive|2017|    1|   4| 26|  14|\n",
      "|2017-01-26|1970-01-01 08:55:03|of jobs and compa...|        -0.5994|       Negative|2017|    1|   4| 26|   8|\n",
      "|2017-01-26|1970-01-01 08:51:46|the us has a 60 b...|        -0.4019|       Negative|2017|    1|   4| 26|   8|\n",
      "|2017-01-26|1970-01-01 06:04:24|ungrateful traito...|        -0.9062|       Negative|2017|    1|   4| 26|   6|\n",
      "|2017-01-25|1970-01-01 21:48:25|interview with da...|         0.4939|       Positive|2017|    1|   4| 25|  21|\n",
      "|2017-01-25|1970-01-01 21:45:32|first interview s...|            0.0|        Neutral|2017|    1|   4| 25|  21|\n",
      "|2017-01-25|1970-01-01 21:14:56|as your president...|         0.1027|       Positive|2017|    1|   4| 25|  21|\n",
      "|2017-01-25|1970-01-01 19:03:33|beginning today t...|         0.4215|       Positive|2017|    1|   4| 25|  19|\n",
      "|2017-01-25|1970-01-01 17:05:59|i will be intervi...|            0.0|        Neutral|2017|    1|   4| 25|  17|\n",
      "|2017-01-25|1970-01-01 07:17:01|i will be making ...|         0.5574|       Positive|2017|    1|   4| 25|   7|\n",
      "|2017-01-25|1970-01-01 07:13:46|even those regist...|        -0.4588|       Negative|2017|    1|   4| 25|   7|\n",
      "+----------+-------------------+--------------------+---------------+---------------+----+-----+----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, weekofyear, dayofmonth, hour, to_date, to_timestamp\n",
    "#from pyspark.sql.functions import to_date, to_timestamp\n",
    "\n",
    "#Convert date column to DateType\n",
    "df = df.withColumn(\"date\", to_date(df[\"date\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "#Convert time column to TimestampType\n",
    "df = df.withColumn(\"time\", to_timestamp(df[\"time\"], \"h:mm:ss a\"))\n",
    "\n",
    "#Extract date components\n",
    "df = df.withColumn(\"year\",year(col(\"Date\")))\n",
    "df = df.withColumn(\"month\",month(col(\"Date\")))\n",
    "df = df.withColumn(\"week\",weekofyear(col(\"Date\")))\n",
    "df = df.withColumn(\"day\",dayofmonth(col(\"Date\")))\n",
    "\n",
    "#Extract time components\n",
    "df = df.withColumn(\"hour\", hour(col(\"Time\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "255ed9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:===================================================>  (191 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-----+\n",
      "|year|sentiment_label|count|\n",
      "+----+---------------+-----+\n",
      "|2009|       Negative|   35|\n",
      "|2009|        Neutral|  112|\n",
      "|2009|       Positive|  245|\n",
      "|2010|       Negative|   70|\n",
      "|2010|        Neutral|  210|\n",
      "|2010|       Positive|  714|\n",
      "|2011|       Negative| 2037|\n",
      "|2011|        Neutral| 1393|\n",
      "|2011|       Positive| 1988|\n",
      "|2012|       Negative| 8162|\n",
      "|2012|        Neutral| 4186|\n",
      "|2012|       Positive|12369|\n",
      "|2013|       Negative|13384|\n",
      "|2013|        Neutral| 7973|\n",
      "|2013|       Positive|35651|\n",
      "|2014|       Negative| 7903|\n",
      "|2014|        Neutral| 6818|\n",
      "|2014|       Positive|25823|\n",
      "|2015|       Negative|10395|\n",
      "|2015|        Neutral|11137|\n",
      "+----+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:=====================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sentiment_distribution_year = df.groupBy(\"year\",\"sentiment_label\").count().orderBy(\"year\",\"sentiment_label\")\n",
    "sentiment_distribution_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29d826d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------+\n",
      "|year|average_tweets_per_week|\n",
      "+----+-----------------------+\n",
      "|2009|                   14.0|\n",
      "|2010|     20.285714285714285|\n",
      "|2011|                 108.36|\n",
      "|2012|      475.3269230769231|\n",
      "|2013|     1096.3076923076924|\n",
      "|2014|      779.6923076923077|\n",
      "|2015|      994.5283018867924|\n",
      "|2016|      558.0188679245283|\n",
      "|2017|                  267.4|\n",
      "+----+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|average_length_per_tweet|\n",
      "+------------------------+\n",
      "|       14.43422741484285|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, split, size\n",
    "\n",
    "# Calculate average tweets per week\n",
    "weekly_tweets = df.groupBy(\"year\", \"week\").count()\n",
    "average_tweets_per_week = weekly_tweets.groupBy(\"year\").agg(avg(\"count\").alias(\"average_tweets_per_week\"))\n",
    "average_tweets_per_week = average_tweets_per_week.orderBy(\"year\")\n",
    "average_tweets_per_week.show()\n",
    "\n",
    "# Calculate average length per tweet (by word)\n",
    "df_with_word_count = df.withColumn(\"word_count\", size(split(\"clean_tweet\", \" \")))\n",
    "average_length_per_tweet = df_with_word_count.agg(avg(\"word_count\").alias(\"average_length_per_tweet\"))\n",
    "average_length_per_tweet.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9505fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra driver is not installed.\n"
     ]
    }
   ],
   "source": [
    "import cassandra\n",
    "\n",
    "# Check if cassandra-driver is installed\n",
    "if 'cassandra' in dir(cassandra):\n",
    "    print(\"Cassandra driver is installed.\")\n",
    "else:\n",
    "    print(\"Cassandra driver is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caba39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1deb428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.cassandra.connection.host\", \"127.0.0.1\")  # Replace with your Cassandra host\n",
    "spark.conf.set(\"spark.cassandra.connection.port\", \"9042\")  # Replace with your Cassandra port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52e7cdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"date_extracted_db\", keyspace=\"my_keyspace\") \\\n",
    "    .mode(\"Append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a512a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
